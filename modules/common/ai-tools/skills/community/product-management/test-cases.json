[
  {
    "ruleId": "",
    "ruleTitle": "Company Research",
    "type": "bad",
    "code": "## Salesforce Competitive Analysis\n\n- CRM software company\n- Founded 1999, public company\n- Products: Sales Cloud, Service Cloud, Marketing Cloud\n- Competitors: HubSpot, Microsoft Dynamics\n- Revenue: ~$31B",
    "language": "markdown",
    "description": "surface-level, no insight"
  },
  {
    "ruleId": "",
    "ruleTitle": "Company Research",
    "type": "good",
    "code": "## Salesforce Competitive Analysis\n\n**Business Model:** Platform lock-in via deep CRM data + AppExchange ecosystem.\nRevenue = subscriptions (80%) + professional services (20%). NRR ~115%.\n\n**Market Position:** Enterprise default for complex sales orgs. Losing SMB to HubSpot\non ease-of-use. Defending with Starter Suite (simplified UX) + AI (Einstein).\n\n**Product Strategy:** Acquisitions to extend data surface (Slack, Tableau, MuleSoft).\nBet: AI requires unified data — own the data layer to own AI outcomes.\n\n**Competitive Dynamics:** HubSpot attacks from below on UX/price. Microsoft attacks\nfrom above via M365 bundle. Primary moat is switching cost, not product superiority.\n\n**Strategic Implication for Us:** SMB segment is contested. Competing on simplicity\nbeats competing on features. Avoid the enterprise segment without differentiated data.",
    "language": "markdown",
    "description": "business model + strategic dynamics"
  },
  {
    "ruleId": "",
    "ruleTitle": "Customer Journey Map",
    "type": "bad",
    "code": "## User Journey: Purchase Flow\n\nStage 1: Awareness → User sees ad\nStage 2: Consideration → User visits website\nStage 3: Purchase → User buys product\nStage 4: Delivery → User receives item\n\nPain point: Checkout is slow.",
    "language": "markdown",
    "description": "missing emotional layer, too generic"
  },
  {
    "ruleId": "",
    "ruleTitle": "Customer Journey Map",
    "type": "good",
    "code": "## Journey Map: First-Time Buyer | Goal: Purchase a gift under $50\n\n### Stage 1: Awareness\n- **Action:** Sees Instagram ad, taps to website\n- **Thought:** \"Do they have something for my sister?\"\n- **Feeling:** Curious, slightly skeptical (2/5)\n- **Pain Point:** No gift category or occasion filter visible\n- **Opportunity:** Add \"Shop by Occasion\" entry point on landing page\n\n### Stage 2: Browse\n- **Action:** Searches \"birthday,\" scans 40+ results\n- **Thought:** \"How do I know what's good quality?\"\n- **Feeling:** Overwhelmed, anxious (1/5) — emotional LOW POINT\n- **Pain Point:** No social proof at browse level; reviews buried on PDPs\n- **Opportunity:** Surface star ratings + \"bestseller\" tags in grid view\n\n### Stage 3: Purchase\n- **Action:** Adds item, begins checkout\n- **Thought:** \"I hope this arrives in time.\"\n- **Feeling:** Cautiously optimistic (3/5)\n- **Pain Point:** Estimated delivery shown only after payment\n- **Opportunity:** Show delivery estimate on PDP and cart",
    "language": "markdown",
    "description": "NNGroup structure with emotional truth"
  },
  {
    "ruleId": "",
    "ruleTitle": "Customer Journey Mapping Workshop",
    "type": "bad",
    "code": "## Journey Mapping Workshop Agenda\n\n9:00 — PM presents current journey map to team\n9:30 — Team reviews and adds sticky notes\n10:00 — PM synthesizes feedback\n10:30 — Final map shared in Confluence\n\nOutcome: Updated journey map document",
    "language": "markdown",
    "description": "presenter mode, no real facilitation"
  },
  {
    "ruleId": "",
    "ruleTitle": "Customer Journey Mapping Workshop",
    "type": "good",
    "code": "## Journey Mapping Workshop | 90 min | Cross-functional team\n\n**Setup (10 min):** Agree on persona + goal. Write it on the board.\n\"We are mapping [Persona] trying to [Goal]. Not all journeys. This one.\"\n\n**Stage Excavation (60 min):** For each stage, facilitator asks in order:\n1. \"What is the customer actually DOING at this point? Physical action?\"\n2. \"What are they thinking — what question is in their head?\"\n3. \"How are they feeling? Give it a number 1–5.\" (Write it large.)\n4. \"Where does our team lose the thread here — who owns this moment?\"\n5. \"What would make this a 5/5 moment? Be specific.\"\n\n**Divergence prompt (if team agrees too fast):**\n\"That was fast — who sees this stage differently? What did we miss?\"\n\n**Synthesis (20 min):** Circle the two lowest emotional scores.\n\"These are our design targets. What do we commit to testing first?\"\n\n**Output:** Jointly owned map + 2 prioritized pain points with named owners.",
    "language": "markdown",
    "description": "adaptive facilitation with stage-by-stage questions"
  },
  {
    "ruleId": "",
    "ruleTitle": "Discovery Interview Prep",
    "type": "bad",
    "code": "## Interview Guide: Project Management Tool\n\n1. Would you use a tool that automatically prioritizes your tasks?\n2. How important is AI-powered scheduling to you? (1–5)\n3. Do you think our dashboard concept would help your team?\n4. What features would you want in an ideal tool?\n5. Would you pay $20/month for this?",
    "language": "markdown",
    "description": "leading questions, hypothetical-focused"
  },
  {
    "ruleId": "",
    "ruleTitle": "Discovery Interview Prep",
    "type": "good",
    "code": "## Interview Guide: Project Management | Goal: Understand task overload problem\n\n**Opening (set context, not pitch):**\n\"I'm trying to understand how you manage your work — not sell you anything.\nTell me about last week. What did your Monday morning look like?\"\n\n**Behavior questions (specific, past-tense):**\n- \"Walk me through the last time a deadline caught you off guard. What happened?\"\n- \"When you sit down to start work, how do you decide what to do first?\"\n- \"What did you actually do the last time you felt overwhelmed by your task list?\"\n- \"What tools were open on your screen at that moment?\"\n\n**Dig on pain (not validation-seeking):**\n- \"How often does that happen?\" (frequency = severity proxy)\n- \"What have you tried to fix it?\" (existing solutions reveal real motivation)\n- \"Why didn't that work?\" (gaps reveal the actual problem)\n\n**Close:**\n\"Is there anything I didn't ask that I should have?\"\n\n**Do NOT ask:** \"Would you use X?\" / \"What features do you want?\" / \"Is this a problem?\"",
    "language": "markdown",
    "description": "Mom Test-compliant, behavior-anchored"
  },
  {
    "ruleId": "",
    "ruleTitle": "Discovery Process",
    "type": "bad",
    "code": "## Q2 Discovery\n\nWeek 1–2: Talk to customers\nWeek 3: Write up notes\nWeek 4: Share findings with team\n\nOutput: Slide deck with quotes\nNext step: Start building the most-requested feature",
    "language": "markdown",
    "description": "undifferentiated \"research sprint,\" no structure"
  },
  {
    "ruleId": "",
    "ruleTitle": "Discovery Process",
    "type": "good",
    "code": "## Discovery Process: [Opportunity Name] | 4 weeks\n\n### Phase 1: Frame (Days 1–3)\n- Write the problem hypothesis: \"We believe [persona] struggles with [problem]\n  in [context], which causes [consequence].\"\n- Define research questions (max 5). What must be true for this to be worth building?\n- Identify participant types needed. Recruit 8–12 people.\n- **Exit criterion:** Team alignment on problem hypothesis + research questions.\n\n### Phase 2: Research (Days 4–14)\n- Run discovery interviews (Mom Test protocol) — 8 minimum.\n- Observe in context where possible (1 session = 3 interviews of insight).\n- Log raw observations, not interpretations, during sessions.\n- **Exit criterion:** No new themes emerging after last 3 interviews (saturation).\n\n### Phase 3: Synthesize (Days 15–17)\n- Affinity cluster across all sessions independently, then together.\n- Identify patterns: frequency × severity matrix for each pain point.\n- Write 3–5 insight statements: \"When [context], [persona] [behavior] because [belief].\"\n- **Exit criterion:** Insights reviewed and challenged by non-research team member.\n\n### Phase 4: Validate (Days 18–21)\n- Map top insights to opportunity hypothesis.\n- Identify the ONE riskiest assumption. Design a PoL probe to test it.\n- Decision gate: proceed / pivot / park — with documented rationale.\n- **Exit criterion:** Go/no-go decision made with evidence, not conviction.",
    "language": "markdown",
    "description": "phased discovery with exit criteria"
  },
  {
    "ruleId": "",
    "ruleTitle": "Jobs to Be Done",
    "type": "bad",
    "code": "## Customer Needs Analysis\n\nUsers want:\n- Better search functionality\n- Faster load times\n- Dark mode\n- Mobile app\n- More integrations with Slack and Notion\n\nPriority: Build the most-requested features first.",
    "language": "markdown",
    "description": "feature-centric, no job context"
  },
  {
    "ruleId": "",
    "ruleTitle": "Jobs to Be Done",
    "type": "good",
    "code": "## Jobs to Be Done: Project Management for Independent Consultants\n\n**Functional Job:**\n\"When I'm about to start my week, I want to see exactly what I must deliver\nand to whom, so I can protect client relationships without keeping it all in my head.\"\n\n**Emotional Job:**\n\"When I'm juggling multiple clients, I want to feel in control (not reactive),\nso I can end the day feeling like a competent professional, not someone who's drowning.\"\n\n**Social Job:**\n\"When a client asks for a status update, I want to respond immediately with specifics,\nso I appear organized and trustworthy — not like I'm scrambling to check notes.\"\n\n**Job Insight:**\nThe core job is not 'task management' — it's 'professional credibility maintenance.'\nFeatures that make the consultant look good to clients outrank features that\nhelp them manage tasks internally. Prioritize: client-facing status views, fast\nupdate sharing, professional-looking exports. Deprioritize: internal kanban improvements.",
    "language": "markdown",
    "description": "JTBD with all three dimensions"
  },
  {
    "ruleId": "",
    "ruleTitle": "Proto Persona",
    "type": "bad",
    "code": "## Target User\n\nOur target user is a busy professional in their 30s who needs to save time.\nThey are tech-savvy and frustrated with their current tools.\nThey want something simple and fast.\n\nWe will build for them.",
    "language": "markdown",
    "description": "undocumented assumptions, no hypothesis framing"
  },
  {
    "ruleId": "",
    "ruleTitle": "Proto Persona",
    "type": "good",
    "code": "## Proto Persona [HYPOTHESIS — not yet validated]\n\n**Name:** Marcus, 34 | Operations Manager, 50-person SaaS company\n\n**Demographics:** 8 years in ops roles. Non-technical but data-literate.\nManages a team of 6. Reports to COO.\n\n**Behaviors (assumed):**\n- Lives in spreadsheets and Slack; no dedicated ops tooling\n- Spends ~4 hrs/week manually compiling status reports for leadership\n- Evaluates new tools alone before proposing to team\n\n**Goals:**\n- Surface operational bottlenecks before the COO asks about them\n- Reduce time on reporting without losing visibility or accuracy\n\n**Frustrations:**\n- Current tools require engineering support to configure\n- Data is scattered across 5+ systems with no single source of truth\n\n**Key Assumptions to Test:**\n1. Marcus is the buyer AND the primary user (not separate roles)\n2. Reporting time (4 hrs/week) is the pain — not the underlying data access\n3. He evaluates tools solo before involving others\n\n**Research Questions for Validation:**\n- How does he currently compile reports? What's the actual workflow?\n- Who else is involved in the tool evaluation/buying decision?\n- What has he tried before and why did it fail?",
    "language": "markdown",
    "description": "explicit hypothesis artifact with testable dimensions"
  },
  {
    "ruleId": "",
    "ruleTitle": "Organizational AI Readiness Assessment",
    "type": "bad",
    "code": "## AI Readiness Check\n\n- Has data team: Yes\n- Uses some ML tools: Yes\n- Leadership is interested in AI: Yes\n\nVerdict: Ready for AI. Proceed with implementation.",
    "language": "markdown",
    "description": "binary readiness verdict"
  },
  {
    "ruleId": "",
    "ruleTitle": "Organizational AI Readiness Assessment",
    "type": "good",
    "code": "## AI Readiness Assessment — Acme Corp\n\n| Competency | Level | Evidence | Gap |\n|---|---|---|---|\n| Data Infrastructure | 3 — Defined | Centralized data warehouse, 80% structured | No real-time pipelines; batch only |\n| Model Literacy | 1 — Nascent | Leadership aware of AI; IC teams have no ML exposure | Training needed before deployment |\n| Workflow Integration | 2 — Emerging | 2 pilot automations in ops; no PM ownership | No process for AI feature lifecycle |\n| Governance | 1 — Nascent | No AI policy; no bias review process | Blocker for any customer-facing AI |\n| Change Management | 2 — Emerging | Change comms in place for IT projects | AI-specific change plan absent |\n\n**Overall Maturity: Level 1–2 (Emerging)**\n\n**Blockers Before AI Product Investment:**\n1. Governance: Draft AI use policy and bias review checklist (4 weeks)\n2. Model Literacy: Run 2-day ML fundamentals workshop for PMs and Eng leads\n3. Workflow Integration: Assign AI feature owner before scoping any initiative\n\n**Recommended Entry Point:** Internal productivity AI (low customer risk) while governance matures.\nDo not launch customer-facing AI features until governance reaches Level 3.",
    "language": "markdown",
    "description": "five-competency maturity grid with gaps and sequencing"
  },
  {
    "ruleId": "",
    "ruleTitle": "Context Engineering for AI Systems",
    "type": "bad",
    "code": "## AI Feature Design\n\nSystem prompt includes:\n- Full user profile (2,000 tokens)\n- Entire conversation history (8,000 tokens)\n- All product documentation (12,000 tokens)\n- Current session state (1,500 tokens)\n\nTotal: ~23,500 tokens per request. Model: GPT-4o.\nCost: $0.047/request. At 50K daily requests = $2,350/day = $858K/year.\nOutput quality declining as context window fills.",
    "language": "markdown",
    "description": "context stuffing — everything in one prompt"
  },
  {
    "ruleId": "",
    "ruleTitle": "Context Engineering for AI Systems",
    "type": "good",
    "code": "## Context Engineering Architecture\n\n**Memory Taxonomy:**\n| Tier | Type | Storage | Retrieval | What Goes Here |\n|---|---|---|---|---|\n| In-context | Working memory | LLM prompt | Always present | Current task, immediate history (last 3 turns), system persona |\n| External (semantic) | Long-term episodic | Vector DB | RAG — top-k relevant chunks | User history, past decisions, product docs |\n| External (structured) | Factual state | SQL/KV store | Tool call — precise lookup | Account data, permissions, pricing, feature flags |\n| In-weights | Procedural | Model fine-tune | Implicit | Domain tone, task patterns (baked in) |\n\n**Routing Rules:**\n- User asks about their account → tool call to structured store (never RAG)\n- User references a past conversation → semantic retrieval, top-3 chunks, inject into context\n- General product question → RAG over docs, max 1,500 tokens injected\n\n**Result:** Average prompt: 3,200 tokens. Cost: $0.006/request. Quality consistent.\nDegradation eliminated — context is always purpose-selected, not bulk-pasted.",
    "language": "markdown",
    "description": "tiered memory architecture with deliberate routing"
  },
  {
    "ruleId": "",
    "ruleTitle": "End-of-Life & Deprecation Communication",
    "type": "bad",
    "code": "## Deprecation Notice: Legacy Reports Module\n\nWe are deprecating the Legacy Reports module on March 1st.\nThis is part of our platform modernization initiative to reduce technical debt\nand align with our new architecture strategy.\n\nPlease update your workflows accordingly.",
    "language": "markdown",
    "description": "internal-focused, no migration path"
  },
  {
    "ruleId": "",
    "ruleTitle": "End-of-Life & Deprecation Communication",
    "type": "good",
    "code": "## What's Changing: Legacy Reports Module Retirement\n\n**What is changing:** The Legacy Reports module will be retired on June 30, 2026.\nAfter this date, the module will no longer be accessible and saved report configurations\nwill not transfer automatically.\n\n**Why:** The new Analytics Hub (launched Q1 2025) delivers the same reporting capabilities\nwith real-time data refresh, custom dimensions, and CSV/API export — features Legacy Reports\ncannot support. Maintaining both systems prevents us from improving either.\n\n**Timeline:**\n- Now → March 31: Both systems available. New features added to Analytics Hub only.\n- April 1 → June 29: Legacy Reports read-only. No new reports can be created.\n- June 30: Legacy Reports retired. Data remains accessible via API export for 90 days.\n\n**Migration Path (3 steps, ~30 minutes):**\n1. Export your saved Legacy report list: Settings → Reports → Export Config (JSON)\n2. Import into Analytics Hub: Analytics Hub → Import Legacy Config → follow wizard\n3. Validate outputs match: use our side-by-side comparison guide [link]\n\n**Support Commitment:**\n- Dedicated migration office hours: Tuesdays 2–4pm ET through June (sign up [link])\n- Migration guide with video walkthrough: [link]\n- If you cannot migrate by June 30, contact support for a 60-day extension.\n\nQuestions? Reply to this email or open a ticket tagged \"legacy-reports-migration\".",
    "language": "markdown",
    "description": "customer-centered with timeline, migration, and support"
  },
  {
    "ruleId": "",
    "ruleTitle": "PM Skill Authoring",
    "type": "bad",
    "code": "## New Skill: Competitive Moat Analysis\n\nThis skill helps you analyze competitive moats.\nUse Porter's Five Forces and think about switching costs.\n\nFiles created:\n- rules/comp-moat.md (stub)",
    "language": "markdown",
    "description": "incomplete skill — missing structure and examples"
  },
  {
    "ruleId": "",
    "ruleTitle": "PM Skill Authoring",
    "type": "good",
    "code": "## Authoring Checklist: Competitive Moat Analysis Skill\n\n**Step 1 — Register in _sections.md**\nAdd entry under appropriate section with impact level, prefix, and description.\nPrefix chosen: `cmp`. Section: 2 (Strategy & Positioning).\n\n**Step 2 — Rule file: rules/cmp-moat-analysis.md**\nRequired elements:\n- [x] Frontmatter: title, impact, impactDescription, tags\n- [x] One-paragraph description: what it does, when to use, when NOT to use\n- [x] Incorrect example: realistic bad PM artifact (not a strawman)\n- [x] Correct example: specific, structured, actionable good artifact\n- [x] Reference link: ../references/cmp-moat-analysis-full.md\n\n**Step 3 — Reference file: references/cmp-moat-analysis-full.md**\nRequired sections: Overview, Framework Steps, Output Template, Benchmarks/Heuristics, Related Skills\n\n**Step 4 — Quality Validation**\n- [ ] Incorrect example is a realistic mistake (not obviously bad)\n- [ ] Correct example could be used directly by a PM today\n- [ ] No `with` patterns or scope-leaking instructions in prompts\n- [ ] Tags match existing taxonomy (no invented tags)\n- [ ] Rule file is 30–50 lines\n\n**Step 5 — Update SKILL.md** (if new interactive skill)\nAdd command entry with trigger phrase and mapped rule file.",
    "language": "markdown",
    "description": "complete skill with all required components"
  },
  {
    "ruleId": "",
    "ruleTitle": "Workshop Facilitation Protocol",
    "type": "bad",
    "code": "Let's work on your positioning. Tell me about your product, your users, their\nmain problems, your competitors, your pricing, your differentiation, your\ncompany stage, your team size, and your key metrics. I'll analyze everything\nand give you a positioning statement.",
    "language": "markdown",
    "description": "no entry mode, no structure, floods user with questions"
  },
  {
    "ruleId": "",
    "ruleTitle": "Workshop Facilitation Protocol",
    "type": "good",
    "code": "## Positioning Workshop\n\nI'll guide you to a complete positioning statement using Geoffrey Moore's framework.\nThis takes 6–8 questions. You can exit and resume anytime.\n\n**How do you want to start?**\n\nA) **Guided** — I ask one question at a time, build context as we go (recommended if you're\n   still forming your thinking)\nB) **Context dump** — Paste or describe everything you know; I'll extract what I need and\n   fill gaps with targeted questions\nC) **Best guess** — Tell me your product in one sentence; I'll draft a positioning statement\n   and we'll refine from there\n\n---\n*(User selects A — Guided)*\n\n**[1/6] Target Customer**\nWho is the primary user of your product? Describe their role, company size, and\nthe specific context in which they'd use it.\n\n*(Wait for response before asking question 2)*\n\n---\n*(After each response, show progress label and acknowledge before proceeding)*\n\n**[2/6] Problem**\n*(building on: mid-market finance teams, 50–200 employees)*\n\nWhat is the specific, painful problem this person faces that your product solves?\nDescribe it in terms of their daily experience, not your solution.",
    "language": "markdown",
    "description": "heads-up + entry mode + one-question-at-a-time + progress labels"
  },
  {
    "ruleId": "",
    "ruleTitle": "Business Health Diagnosis",
    "type": "bad",
    "code": "## Q3 Health Check\n\nRevenue grew 35% YoY. Business is healthy.\n\nRecommendation: Accelerate hiring and marketing spend.",
    "language": "markdown",
    "description": "single-metric verdict"
  },
  {
    "ruleId": "",
    "ruleTitle": "Business Health Diagnosis",
    "type": "good",
    "code": "## Q3 Health Diagnosis\n\n**Revenue Growth:** 35% YoY — strong, but decelerating from 52% in Q2. Watch for S-curve plateau.\n\n**Retention:** Gross churn 18% annualized, NRR 94% — RED FLAG. Expansion cannot offset losses.\nChurn source: SMB segment (>60% of churned logos). Root cause: time-to-value >90 days.\n\n**Unit Economics:** CAC payback 28 months, LTV:CAC 2.1x — below 3x floor. Sales efficiency declining.\nMagic number: 0.6 (threshold is 0.75). Every dollar of S&M is underperforming.\n\n**Cash Efficiency:** Burn multiple 2.4x — burning $2.40 to generate $1 of ARR. Unsustainable at scale.\n\n**Priority Actions:**\n1. Fix SMB onboarding to reduce time-to-value below 30 days (churn lever)\n2. Pause bottom-funnel spend until magic number exceeds 0.75 (efficiency lever)\n3. Model runway at current burn — extend to 18+ months before next growth push",
    "language": "markdown",
    "description": "multi-dimensional diagnosis with red flags"
  },
  {
    "ruleId": "",
    "ruleTitle": "SaaS Finance Metrics Reference",
    "type": "bad",
    "code": "## Our Key Metrics\n\n- MRR: $420K\n- Churn: 5%\n- LTV: $18K\n- CAC: $6K\n- Rule of 40: 38\n\nStatus: Good",
    "language": "markdown",
    "description": "metric without formula or benchmark context"
  },
  {
    "ruleId": "",
    "ruleTitle": "SaaS Finance Metrics Reference",
    "type": "good",
    "code": "## Key SaaS Metrics — Q3 Snapshot\n\n| Metric | Formula | Value | Benchmark | Signal |\n|---|---|---|---|---|\n| MRR | Sum of all recurring monthly revenue | $420K | — | Growing 8% MoM |\n| ARR | MRR × 12 | $5.04M | — | — |\n| Gross Churn | Churned MRR / Beginning MRR | 2.1%/mo (23% ann.) | <5% ann. SMB | RED: above threshold |\n| Net Revenue Retention | (Beg. MRR + Expansion − Churn − Contraction) / Beg. MRR | 97% | >100% good, >120% elite | WARN: below 100% |\n| CAC | Total S&M spend / New customers acquired | $6,200 | Varies by segment | Blended |\n| CAC Payback | CAC / (ARPU × Gross Margin %) | 22 months | <12 best, <18 good | WARN |\n| LTV | ARPU × Gross Margin % / Churn Rate | $18K | LTV:CAC >3x target | 2.9x — borderline |\n| Rule of 40 | Revenue growth % + Profit margin % | 38 | ≥40 healthy | WARN |\n| Magic Number | Net new ARR / S&M spend prior quarter | 0.71 | >0.75 efficient | WARN |\n| Burn Multiple | Net burn / Net new ARR | 1.8x | <1x elite, <2x ok | Acceptable |\n\n**Interpretation:** Churn is the dominant risk. All efficiency metrics are borderline because gross retention is dragging NRR below 100%. Fix retention before scaling spend.",
    "language": "markdown",
    "description": "metric with formula, benchmark, and interpretation"
  },
  {
    "ruleId": "",
    "ruleTitle": "Pricing Change Financial Analysis",
    "type": "bad",
    "code": "## Pricing Update Proposal\n\nOur current Pro plan at $99/mo feels too expensive based on sales feedback.\nProposal: Drop Pro to $79/mo to increase conversion.\n\nExpected outcome: More signups, higher revenue.",
    "language": "markdown",
    "description": "intuition-driven, no model"
  },
  {
    "ruleId": "",
    "ruleTitle": "Pricing Change Financial Analysis",
    "type": "good",
    "code": "## Pricing Change Analysis — Pro Plan: $99 → $79/mo\n\n**Baseline:** 1,200 Pro subscribers × $99 = $118,800 MRR\n\n**Scenario Modeling (price elasticity estimate: 1.4):**\n| Scenario | New Subscribers | Churned | Net MRR | Delta |\n|---|---|---|---|---|\n| Bear (-10% volume gain) | +120 | 0 | $116,280 | -$2,520 (-2%) |\n| Base (+18% volume gain) | +216 | 0 | $115,104 | -$3,696 (-3%) |\n| Bull (+30% volume gain) | +360 | 0 | $113,880 | -$4,920 (-4%) |\n\n**Finding:** At elasticity 1.4, a 20% price cut requires 28% volume gain just to break even on MRR.\nCurrent conversion bottleneck is onboarding friction, not price — elasticity assumption is unvalidated.\n\n**Cannibalization Risk:** Starter plan at $39/mo. If Pro drops to $79, price gap narrows to 2x.\nRisk: ~15% of current Enterprise trials downgrade to Pro. Estimated impact: -$22K MRR.\n\n**Competitive Position:** Pro at $99 is at market median. Cutting to $79 signals value uncertainty,\nnot competitiveness. Recommend testing friction removal before price reduction.\n\n**Recommendation:** Do not reduce price. Run 60-day onboarding experiment first; re-evaluate if\nconversion rate does not improve by ≥20%.\n\n**Success Criteria (if experiment proceeds):** MRR neutral within 90 days, NRR improvement ≥2%.",
    "language": "markdown",
    "description": "impact model with elasticity and risk assessment"
  },
  {
    "ruleId": "",
    "ruleTitle": "SaaS Unit Economics & Capital Efficiency",
    "type": "bad",
    "code": "## Unit Economics Summary\n\n- CAC: $8,400\n- LTV: $29,000\n- LTV:CAC: 3.5x\n- Gross margin: 71%\n\nVerdict: Healthy unit economics. Recommend increasing sales headcount.",
    "language": "markdown",
    "description": "metrics reported without efficiency context"
  },
  {
    "ruleId": "",
    "ruleTitle": "SaaS Unit Economics & Capital Efficiency",
    "type": "good",
    "code": "## Unit Economics & Capital Efficiency Audit\n\n**CAC & Payback**\n- Blended CAC: $8,400 (Sales-touch: $14,200 | Self-serve: $1,100)\n- CAC Payback: 31 months (Sales-touch) — RED: threshold is <18 months\n- CAC Payback: 8 months (Self-serve) — GREEN: excellent\n- Implication: Sales-touch channel is capital-inefficient at current close rates\n\n**LTV:CAC**\n- LTV: $29,000 | LTV:CAC: 3.5x — borderline (target: >3x; elite: >5x)\n- WARNING: LTV is sensitive to 23% gross churn. If churn rises 5pp, LTV drops to $21K → 2.5x\n\n**Gross Margin**\n- Current: 71% — approaching acceptable floor (70%). Investigate COGS drivers.\n- Benchmark: Best-in-class SaaS 75–85%. Gap suggests hosting or CS cost bloat.\n\n**Capital Efficiency**\n- Burn Multiple: 2.2x — burning $2.20 per $1 of net new ARR (target <1.5x)\n- Magic Number: 0.68 — below 0.75 efficiency threshold\n- Rule of 40: Growth 28% + FCF margin -14% = 14 — FAR below 40 threshold\n\n**Verdict:** Self-serve is the healthy core. Sales-touch is destroying capital efficiency.\nRecommend: shift investment to self-serve PLG motion; redesign sales comp to target\n≤18-month payback accounts only. Do not scale headcount until magic number exceeds 0.75.",
    "language": "markdown",
    "description": "system view with thresholds and capital efficiency diagnosis"
  },
  {
    "ruleId": "",
    "ruleTitle": "SaaS Revenue Health Metrics",
    "type": "bad",
    "code": "## Revenue Update — Q3\n\nARR: $6.2M (up 41% YoY). Great quarter.\nNew logos: 48. Average deal: $129K ACV.\n\nOutlook: On track to hit $8M ARR by year-end.",
    "language": "markdown",
    "description": "headline metric without retention breakdown"
  },
  {
    "ruleId": "",
    "ruleTitle": "SaaS Revenue Health Metrics",
    "type": "good",
    "code": "## Revenue Health — Q3 MRR Bridge\n\n**Beginning MRR:** $483,000\n\n| Movement | Amount | Notes |\n|---|---|---|\n| New Business | +$52,000 | 48 logos, avg $1,083 MRR |\n| Expansion | +$18,400 | Seat additions + plan upgrades |\n| Contraction | -$9,200 | Downgrades, seat reductions |\n| Churn | -$31,600 | 29 logos lost |\n| **Ending MRR** | **$512,600** | +6.1% MoM |\n\n**Gross MRR Churn:** $31,600 / $483,000 = 6.5%/mo — RED (annualized 78%; target <5%/yr)\n**Net MRR Churn:** ($31,600 + $9,200 − $18,400) / $483,000 = 4.6%/mo — RED\n\n**NRR (trailing 12 months):** 88% — below 100% floor. Business shrinks without new logos.\n\n**Expansion Rate:** $18,400 / $483,000 = 3.8% — healthy expansion motion; cannot offset gross churn\n\n**Cohort Analysis Signal:** Jan cohort (12 months old) at 61% logo retention, 74% revenue retention.\nExpansion partially masking logo churn. Cohorts are NOT improving — Jan vs. Jul cohorts within 2pp.\n\n**Root Cause Hypothesis:** Churn concentrated in <$500 MRR accounts (68% of churned logos).\nSMB segment requires self-serve success motion — current high-touch CS model is uneconomic there.\n\n**Priority:** Segment churn analysis by ACV, industry, and onboarding path before Q4 planning.",
    "language": "markdown",
    "description": "full MRR bridge with retention and cohort signal"
  },
  {
    "ruleId": "",
    "ruleTitle": "Acquisition Channel Evaluation",
    "type": "bad",
    "code": "## Q3 Growth Channels\n\n- Google Ads — performing well, keep running\n- LinkedIn Ads — expensive but good leads\n- Content SEO — slow but building\n- Referral program — launched last quarter\n- Cold outbound — SDR team is working it\n\nAction: increase budget across the board",
    "language": "markdown",
    "description": "channel list with no economics, no verdict"
  },
  {
    "ruleId": "",
    "ruleTitle": "Acquisition Channel Evaluation",
    "type": "good",
    "code": "## Q3 Acquisition Channel Evaluation\n\n| Channel       | CAC    | Conv. Rate | Payback Period | Verdict |\n|---------------|--------|------------|----------------|---------|\n| Google Ads    | $420   | 3.2%       | 7 months       | SCALE   |\n| LinkedIn Ads  | $1,800 | 1.1%       | 30 months      | KILL    |\n| Content SEO   | $210   | 2.8%       | 4 months       | SCALE   |\n| Referral      | $95    | 8.4%       | 2 months       | SCALE   |\n| Cold outbound | $640   | 0.9%       | 11 months      | TEST    |\n\nVerdicts:\n- SCALE: CAC payback < 12 months and conv. rate above category benchmark\n- TEST: Payback 12–18 months; run 60-day capped experiment before committing\n- KILL: Payback > 18 months or conv. rate below breakeven threshold\n\nQ3 budget reallocation: shift 60% of LinkedIn spend to Referral and SEO.",
    "language": "markdown",
    "description": "channel-by-channel unit economics with scale/test/kill verdict"
  },
  {
    "ruleId": "",
    "ruleTitle": "Feature Investment Analysis",
    "type": "bad",
    "code": "## Feature: Advanced Reporting\n\nWhy build it:\n- Customers have been asking for this\n- Competitors have it\n- Will help with retention\n\nDecision: Yes, let's build it.\nTimeline: Q3",
    "language": "markdown",
    "description": "vague benefit claim, no investment sizing"
  },
  {
    "ruleId": "",
    "ruleTitle": "Feature Investment Analysis",
    "type": "good",
    "code": "## Feature Investment: Advanced Reporting\n\n### Investment\n- Engineering effort: 6 weeks (2 engineers)\n- Design effort: 2 weeks\n- Total cost (fully-loaded): ~$48,000\n\n### Expected Return (12-month horizon)\n- Retention impact: closes reported reason for 18% of churned accounts\n  → 12 accounts/yr × $8,400 ACV = $100,800 retained ARR\n- Expansion: enables upsell to Analytics tier for 30 existing accounts\n  → 30 × $1,200 incremental MRR × 12 = $43,200\n- Total 12-month return: ~$144,000\n\n### ROI Ratio: 3.0x (return / investment cost)\n\n### Strategic Value\n- Directly supports \"data-first operations\" positioning differentiator\n- Closes primary feature gap vs. Competitor A (cited in 9 loss notes Q1)\n\n### Verdict: BUILD\nThreshold for build: ROI > 2x AND supports current positioning.\nBoth conditions met. Schedule for Q3, cap at 8 weeks total.",
    "language": "markdown",
    "description": "quantified ROI, strategic value, and explicit build/don't-build verdict"
  },
  {
    "ruleId": "",
    "ruleTitle": "PESTEL Analysis",
    "type": "bad",
    "code": "## PESTEL — HR Tech Product\n\nPolitical: government regulations changing\nEconomic: recession concerns\nSocial: remote work trends continuing\nTechnological: AI is advancing fast\nEnvironmental: sustainability matters more\nLegal: GDPR and data privacy rules\n\nConclusion: lots of external factors to watch",
    "language": "markdown",
    "description": "factor list with no impact mapping or strategic response"
  },
  {
    "ruleId": "",
    "ruleTitle": "PESTEL Analysis",
    "type": "good",
    "code": "## PESTEL Analysis — HR Tech Product (Q1 2026)\n\n| Factor        | Specific Force                          | Impact | Likelihood | Strategic Implication                              |\n|---------------|-----------------------------------------|--------|------------|----------------------------------------------------|\n| Political     | EU AI Act enforcement begins H2 2026    | High   | Certain    | Audit automated decision features before June      |\n| Economic      | SMB hiring freeze in core segment       | High   | Probable   | Shift ICP toward enterprise; revisit pricing tiers |\n| Social        | 4-day workweek adoption rising in EU    | Medium | Probable   | Add scheduling flexibility to roadmap backlog      |\n| Technological | LLM cost dropping 10x every 18 months   | High   | Certain    | Accelerate AI-assisted workflows; revisit build/buy|\n| Environmental | ESG reporting now expected by investors | Low    | Probable   | No product action; monitor for enterprise segment  |\n| Legal         | GDPR Article 22 on automated decisions  | High   | Certain    | Legal review required before Q3 product launch     |\n\nPrioritized actions:\n1. Legal: schedule EU AI Act compliance review (owner: PM + Legal, due May)\n2. Economic: update ICP definition and CAC model for enterprise segment (due Q2 planning)\n3. Technological: spike on LLM-assisted offer letter generation (2-week timebox)",
    "language": "markdown",
    "description": "factor mapped to impact, likelihood, and strategic implication"
  },
  {
    "ruleId": "",
    "ruleTitle": "Prioritization Framework Advisor",
    "type": "bad",
    "code": "## Q2 Prioritization\n\nWe used RICE to score the backlog but estimates were rough guesses.\nAlso added MoSCoW labels and sorted by Kano category.\n\nTop items:\n1. Feature A — RICE: 2,400 (must-have, delighter)\n2. Feature B — RICE: 1,800 (should-have, performance)\n3. Feature C — RICE: 1,200 (could-have, basic)\n\nUsed confidence: 100% for all items.",
    "language": "markdown",
    "description": "framework mismatch, mixed signals, no selection rationale"
  },
  {
    "ruleId": "",
    "ruleTitle": "Prioritization Framework Advisor",
    "type": "good",
    "code": "## Q2 Prioritization — Framework Selection\n\nContext:\n- Stage: Post-PMF, scaling (18 months post-launch, product instrumented)\n- Data maturity: Quantitative (analytics, NPS, support volume available)\n- Decision context: Internal team ranking, not stakeholder communication\n\nRecommended framework: RICE\nRationale: Scaling stage with quantitative data makes Reach and Impact\nestimable. Confidence score forces explicit uncertainty acknowledgment.\n\nIf context were different:\n- Pre-PMF or no analytics → use ICE (faster, less data-dependent)\n- Feature discovery with user research → use Kano survey\n- Sprint scope negotiation with stakeholders → use MoSCoW\n\n## Q2 RICE Scores\n\n| Feature          | Reach | Impact | Confidence | Effort | RICE  |\n|------------------|-------|--------|------------|--------|-------|\n| Bulk export      | 4,200 | 2      | 80%        | 3 wks  | 2,240 |\n| SSO integration  | 1,800 | 3      | 60%        | 6 wks  | 540   |\n| Email digest     | 6,000 | 1      | 90%        | 1 wk   | 5,400 |\n\nConfidence: % based on supporting evidence (interviews, data, analogues).\nDo not default to 100% — no estimate deserves it.",
    "language": "markdown",
    "description": "framework selected based on context, applied consistently"
  },
  {
    "ruleId": "",
    "ruleTitle": "Roadmap Planning",
    "type": "bad",
    "code": "## 2026 Product Roadmap\n\nJanuary: Dark mode\nFebruary: Mobile app redesign\nMarch: Zapier integration\nApril: Advanced search filters\nMay: Team permissions v2\nJune: API v3\n\nNote: dates are committed to sales team and on website.",
    "language": "markdown",
    "description": "feature list with fixed dates, no strategic link"
  },
  {
    "ruleId": "",
    "ruleTitle": "Roadmap Planning",
    "type": "good",
    "code": "## Q2 2026 Roadmap — Last updated: Feb 21\n\n### Inputs gathered\n- Strategy: \"data-first operations\" positioning (see Positioning Statement v3)\n- Validated problems: report navigation friction (#1 support theme, 180 tickets/mo)\n- Capacity: 3 engineers × 10 weeks = 30 eng-weeks available\n- Stakeholder asks: Sales requests SSO (7 enterprise deals blocked)\n\n### Initiatives\n\n**NOW (this quarter, committed)**\n- Initiative: Reduce report navigation friction\n  Outcome: avg. navigation clicks per session from 8.2 → 4.0\n  Epics: persistent sidebar, keyboard shortcuts, recent history\n  Capacity: 12 eng-weeks | Owner: Ana\n\n- Initiative: Unblock enterprise sales via SSO\n  Outcome: 7 stalled enterprise deals unblocked, SSO as table-stakes feature\n  Epics: SAML 2.0, admin provisioning flow\n  Capacity: 8 eng-weeks | Owner: Ben\n\n**NEXT (next quarter, directional)**\n- Initiative: Self-serve analytics exports\n  Outcome: reduce custom export support tickets by 60%\n  Why next: depends on NOW sidebar work; data model changes required first\n\n**LATER (6+ months, intent only)**\n- Initiative: Mobile companion app (Q4 estimate)\n  Why later: no validated mobile use case yet; discovery planned for Q3\n\n### What is NOT on this roadmap\n- Dark mode: low strategic value, no customer problem evidence\n- Zapier integration: under evaluation; channel ROI analysis pending",
    "language": "markdown",
    "description": "initiative-level, outcome-framed, Now/Next/Later with rationale"
  },
  {
    "ruleId": "",
    "ruleTitle": "Epic Breakdown Advisor",
    "type": "bad",
    "code": "Epic: User can pay for orders\n\nStories:\n- Build payment database schema\n- Build payment API endpoint\n- Build payment UI form\n- Wire up Stripe SDK",
    "language": "markdown",
    "description": "splitting by technical component, not user value"
  },
  {
    "ruleId": "",
    "ruleTitle": "Epic Breakdown Advisor",
    "type": "good",
    "code": "Epic: User can pay for orders\n\nStories:\n- User pays with a saved credit card (happy path)\n- User pays with a new credit card (data variation)\n- User sees an error when payment is declined (unhappy path)\n- User pays with PayPal (interface variation)\n- Admin refunds an order (operation variation)",
    "language": "markdown",
    "description": "splitting by workflow steps and happy/unhappy paths"
  },
  {
    "ruleId": "",
    "ruleTitle": "Epic Hypothesis",
    "type": "bad",
    "code": "Epic: Build an onboarding flow\n\nWe will create a multi-step onboarding wizard for new users\nthat walks them through setting up their profile and preferences.",
    "language": "markdown",
    "description": "solution-framed, no measurable outcome"
  },
  {
    "ruleId": "",
    "ruleTitle": "Epic Hypothesis",
    "type": "good",
    "code": "Epic: Improve new user onboarding\n\nWe believe that providing a guided 3-step setup flow\nfor new users who have not completed their profile\nwill increase 7-day activation rate\nas measured by the percentage of sign-ups who complete\nat least one core action within their first week.\n\nCurrent baseline: 22% | Target: 35%",
    "language": "markdown",
    "description": "hypothesis-framed with audience, action, outcome, and metric"
  },
  {
    "ruleId": "",
    "ruleTitle": "PRD Development",
    "type": "bad",
    "code": "# PRD: Dashboard Redesign\n\n## Overview\nWe will redesign the dashboard with a new sidebar navigation,\nupdated color scheme, and drag-and-drop widget support.\n\n## Features\n- Sidebar nav\n- Dark mode\n- Drag-and-drop widgets",
    "language": "markdown",
    "description": "solution-first, no target user or success metric"
  },
  {
    "ruleId": "",
    "ruleTitle": "PRD Development",
    "type": "good",
    "code": "# PRD: Dashboard Redesign\n\n## Executive Summary\nPower users spend 40% of their session time navigating between\nreports. This PRD proposes a redesigned dashboard to reduce\nnavigation time and increase daily active usage.\n\n## Problem Statement\nUsers with 5+ saved reports cannot find or switch between them\nefficiently. Support tickets cite \"can't find my reports\" as\nthe #2 complaint (180/month).\n\n## Target Users\nOperations analysts at mid-market companies (50–500 employees)\nwho run 3+ reports per day.\n\n## Success Metrics\n- Reduce avg. navigation clicks per session from 8.2 → 4.0\n- Increase DAU/MAU ratio from 0.31 → 0.45 within 60 days post-launch\n\n## Solution Overview\nPersistent sidebar with pinned reports, keyboard shortcuts,\nand recently-viewed history.",
    "language": "markdown",
    "description": "problem-first, structured with target user and measurable success"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story Mapping Workshop",
    "type": "bad",
    "code": "Workshop output:\n\nTeam brainstormed features:\n- Notifications\n- Admin dashboard\n- API integration\n- Mobile app\n- Reporting\n- User roles\n\nNext step: engineering to estimate all items.",
    "language": "markdown",
    "description": "workshop produces a feature list, no backbone or release slices"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story Mapping Workshop",
    "type": "good",
    "code": "Workshop output — Persona: Operations Manager at a logistics company\n\nBackbone Activities:\n[Plan shipment] → [Assign driver] → [Track delivery] → [Resolve issues]\n\nTasks under \"Track delivery\":\n- View real-time map\n- See estimated arrival time\n- Receive delay alerts\n- Share tracking link with customer\n\nStory cards (vertical, priority order):\nTop: View real-time map | See estimated arrival time\nMid: Receive delay alerts\nLower: Share tracking link with customer\n\nRelease slices agreed:\n── Release 1: View map + ETA ──────────────────────────────\n── Release 2: Delay alerts ────────────────────────────────\n── Release 3: Shareable tracking link ─────────────────────",
    "language": "markdown",
    "description": "workshop produces backbone, tasks, stories, and release slices"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story Mapping",
    "type": "bad",
    "code": "Backlog:\n- Add item to cart\n- User registration\n- Email confirmation\n- Search products\n- Checkout\n- Order history\n- Password reset\n- Product detail page",
    "language": "markdown",
    "description": "flat feature list with no workflow context"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story Mapping",
    "type": "good",
    "code": "Backbone (Activities):  Browse  →  Select  →  Purchase  →  Track\n\nTasks:\nBrowse:   Search by keyword | Filter by category | View featured items\nSelect:   View product detail | Read reviews | Check availability\nPurchase: Add to cart | Enter shipping info | Pay | Confirm order\nTrack:    View order status | Receive email update | Request return\n\nRelease 1 (MVP — top row):\nSearch by keyword | View product detail | Add to cart | Pay | Confirm order\n\nRelease 2:\nFilter by category | Read reviews | View order status\n\nRelease 3:\nView featured items | Check availability | Receive email update | Request return",
    "language": "markdown",
    "description": "backbone with activities → tasks, stories tiered by release"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story Splitting",
    "type": "bad",
    "code": "Original: As a manager I want to export reports so that I can share them with stakeholders\n\nSplit:\n- Build export API endpoint\n- Build export UI button\n- Write export to CSV logic\n- Add file download handler",
    "language": "markdown",
    "description": "split by technical layer — stories are dependent and have no standalone value"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story Splitting",
    "type": "good",
    "code": "Original: As a manager I want to export reports so that I can share them with stakeholders\n\nSplit by data type:\n- As a manager I want to export reports as CSV so that I can open them in Excel\n- As a manager I want to export reports as PDF so that I can share a formatted document\n\nSplit by happy/unhappy path:\n- As a manager I want to export a report successfully so that I receive the file\n- As a manager I want to see a clear error if the export fails so that I know to retry\n\nSplit by operation:\n- As a manager I want to export a single report\n- As a manager I want to export all reports in a date range (deferred to sprint N+2)",
    "language": "markdown",
    "description": "split by data type and operation — each story ships independently"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story",
    "type": "bad",
    "code": "Story: Password reset\n\nThe system shall allow users to reset their password via email.\nThe reset link should expire after some time.\nThe user should see a confirmation message.",
    "language": "markdown",
    "description": "system-centric, acceptance criteria are prose, not testable"
  },
  {
    "ruleId": "",
    "ruleTitle": "User Story",
    "type": "good",
    "code": "Story: Reset forgotten password\n\nAs a registered user\nI want to reset my password using my email address\nSo that I can regain access to my account if I forget my credentials\n\nAcceptance Criteria:\n\nGiven I am on the login page\nWhen I click \"Forgot password\" and submit my email\nThen I receive a reset link within 2 minutes\n\nGiven I click a valid reset link\nWhen I enter and confirm a new password\nThen my password is updated and I am redirected to the dashboard\n\nGiven a reset link older than 24 hours\nWhen I attempt to use it\nThen I see an error message and am prompted to request a new link",
    "language": "markdown",
    "description": "role-centric, Gherkin AC, INVEST-compliant"
  },
  {
    "ruleId": "",
    "ruleTitle": "Positioning Statement",
    "type": "bad",
    "code": "For businesses that need better software, our platform is an all-in-one solution\nthat helps teams work smarter. Unlike competitors, we are faster and easier to use.",
    "language": "markdown",
    "description": "generic and unfalsifiable"
  },
  {
    "ruleId": "",
    "ruleTitle": "Positioning Statement",
    "type": "good",
    "code": "For mid-market SaaS finance teams (50–500 employees) that struggle to close the\nbooks in under five days, our product is a financial close automation platform\nthat reduces close time by 40% through automated reconciliation workflows.\nUnlike spreadsheet-based processes and legacy ERP modules, we provide real-time\naudit trails and one-click variance explanations without requiring IT involvement.",
    "language": "markdown",
    "description": "specific, structured, and testable"
  },
  {
    "ruleId": "",
    "ruleTitle": "Positioning Workshop",
    "type": "bad",
    "code": "Workshop output after 30 minutes:\n- Target: \"Enterprise companies\"\n- Problem: \"They need efficiency\"\n- Differentiator: \"We're easier to use and have better support\"\n→ Approved by team. Moving to launch copy.",
    "language": "markdown",
    "description": "skipping discovery phases"
  },
  {
    "ruleId": "",
    "ruleTitle": "Positioning Workshop",
    "type": "good",
    "code": "Phase 1 — Target Market:\nQ: Who specifically buys today? Who rejects us?\nFinding: Primary buyer = VP Finance at Series B SaaS (not IT, not CEO)\n\nPhase 2 — Problem Validation:\nQ: What does the buyer say in their own words?\nFinding: \"I can't trust the numbers my team gives me at month-end\"\n\nPhase 3 — Competitive Alternatives:\nQ: What do they do today without us?\nFinding: Excel + Slack threads + manual Salesforce exports\n\nPhase 4 — Differentiator Test:\nQ: Can we prove this claim with data?\nFinding: 3 case studies show 40% close-time reduction — claim is defensible",
    "language": "markdown",
    "description": "evidence-grounded, phase-by-phase"
  },
  {
    "ruleId": "",
    "ruleTitle": "Press Release (Working Backwards)",
    "type": "bad",
    "code": "# Acme Corp Launches New Dashboard Feature\n\nAcme Corp today announced the release of version 3.2, which includes an improved\ndashboard with 14 new chart types, a redesigned navigation bar, and API rate limit\nincreases. The engineering team worked for six months on these improvements.",
    "language": "markdown",
    "description": "feature-focused, no customer outcome"
  },
  {
    "ruleId": "",
    "ruleTitle": "Press Release (Working Backwards)",
    "type": "good",
    "code": "# Finance Teams at Mid-Market SaaS Companies Cut Month-End Close from 10 Days to 4\n\n[City, Date] — Starting today, finance teams can eliminate the manual reconciliation\nwork that eats two weeks every quarter. CloseLoop's automated variance detection\nflags discrepancies in real time, so controllers spend hours reviewing instead of\ndays hunting.\n\n\"We used to dread the last week of every month,\" said Maria Chen, Controller at\nLaunchpad Analytics. \"Now close is just another Tuesday.\"\n\nAvailable immediately. Start your free 14-day trial at closeloop.com/start.",
    "language": "markdown",
    "description": "outcome-focused, customer-centric"
  },
  {
    "ruleId": "",
    "ruleTitle": "Problem Framing Canvas",
    "type": "bad",
    "code": "Problem: Users can't find the export button.\nSolution: Move the export button to the top navigation.\n→ Team begins design sprint.",
    "language": "markdown",
    "description": "jumping to solution framing"
  },
  {
    "ruleId": "",
    "ruleTitle": "Problem Framing Canvas",
    "type": "good",
    "code": "Look Inward:\n- Assumption: Users want to export data\n- Bias: Engineering team suggested this based on support tickets, not user research\n- Challenge: Are exports the goal, or is sharing data the goal?\n\nLook Outward:\n- Stakeholders: End users (analysts), their managers (recipients of exports)\n- Context: Analysts export to send to managers via email every Friday\n- Constraint: Managers don't have product access\n\nReframe:\nOriginal: \"Users can't find the export button\"\nReframed: \"Analysts need a reliable way to share live data snapshots with\nstakeholders who lack product access — weekly manual exports are a workaround\nfor a missing collaboration feature\"",
    "language": "markdown",
    "description": "three-phase canvas applied"
  },
  {
    "ruleId": "",
    "ruleTitle": "Problem Statement",
    "type": "bad",
    "code": "Problem: The app needs a better onboarding flow because users are dropping off.\nWe should add a tutorial wizard and tooltips.",
    "language": "markdown",
    "description": "solution-framed, persona-free"
  },
  {
    "ruleId": "",
    "ruleTitle": "Problem Statement",
    "type": "good",
    "code": "I am a newly hired revenue operations analyst\ntrying to get my first monthly pipeline report out within my first two weeks\nbut I cannot connect our CRM data to the reporting tool without IT help\nbecause the integration requires admin credentials I'm not authorized to have,\nwhich makes me feel like I'm failing at a basic part of my job before I've even started.\n\nSource: 6/8 onboarding interview participants described a version of this barrier.\nDirect quote: \"I spent three days on what I thought would take an hour.\" — P4",
    "language": "markdown",
    "description": "evidence-backed, structured format"
  },
  {
    "ruleId": "",
    "ruleTitle": "Product Strategy Session",
    "type": "bad",
    "code": "Strategy Day Agenda (8 hours):\n9am  — Brainstorm target customers\n10am — Define differentiators\n11am — Size the market\n1pm  — Build the roadmap\n4pm  — Present to leadership\n\nOutput: 47-slide deck, no shared written artifacts, three conflicting interpretations",
    "language": "markdown",
    "description": "phases collapsed into a single session"
  },
  {
    "ruleId": "",
    "ruleTitle": "Product Strategy Session",
    "type": "good",
    "code": "Week 1: Positioning\n- Run Positioning Workshop → produce signed-off Positioning Statement\n- Artifact: \"For [target] that [need], we are a [category] that [benefit]...\"\n\nWeek 2: Problem Clarity\n- Run Problem Framing Canvas → produce Reframed Problem Statement\n- Run Problem Statement workshop → 3 validated persona-level statements\n\nWeek 3: Market Validation\n- Run TAM/SAM/SOM Calculator → market size with cited methodology\n- Go/no-go gate: Is the SOM large enough to justify the strategy?\n\nWeek 4: Roadmap Planning\n- Translate validated strategy into 12-month roadmap with sequenced bets\n- Each initiative links back to positioning differentiator or problem statement",
    "language": "markdown",
    "description": "sequenced, artifact-driven over multiple weeks"
  },
  {
    "ruleId": "",
    "ruleTitle": "TAM SAM SOM Calculator",
    "type": "bad",
    "code": "Market Sizing:\n- TAM: $50B (the global HR software market)\n- SAM: $10B (mid-market segment, roughly 20%)\n- SOM: $500M (we'll capture 5% in year 3)\n\nSource: \"industry reports\"",
    "language": "markdown",
    "description": "top-down, uncited, no methodology"
  },
  {
    "ruleId": "",
    "ruleTitle": "TAM SAM SOM Calculator",
    "type": "good",
    "code": "TAM — Bottom-up calculation:\n- 180,000 mid-market SaaS companies in the US (source: Crunchbase 2025, companies\n  with $10M–$500M ARR)\n- Average spend on financial close software: $28,000/yr (source: Gartner MQ 2024)\n- TAM = 180,000 × $28,000 = $5.04B\n\nSAM — Segment filter:\n- Target: Series B–D SaaS, finance team 3–15 people, using Salesforce CRM\n- Estimated count: 22,000 companies (source: LinkedIn Sales Navigator export, Feb 2025)\n- SAM = 22,000 × $28,000 = $616M\n\nSOM — Realistic capture:\n- Year 1 target: 80 customers (based on current sales capacity of 4 AEs × 20 deals)\n- Year 3 target: 600 customers (with planned team expansion)\n- SOM = 600 × $24,000 ACV = $14.4M ARR by end of year 3",
    "language": "markdown",
    "description": "bottom-up, cited, with methodology"
  },
  {
    "ruleId": "",
    "ruleTitle": "Lean UX Canvas",
    "type": "bad",
    "code": "## Q3 Initiative: Notifications\n\nSolutions:\n- Add email digest\n- Add in-app badge counter\n- Add push notifications\n\nNext step: Hand off to engineering.",
    "language": "markdown",
    "description": "solutions-first, outcomes missing"
  },
  {
    "ruleId": "",
    "ruleTitle": "Lean UX Canvas",
    "type": "good",
    "code": "## Lean UX Canvas: Notification Overload\n\n**1. Business Problem:** Users are missing critical updates, causing escalations and churn.\n**2. Users:** Power users managing 5+ projects simultaneously.\n**3. User Outcomes:** Feel in control; act on the right information at the right time.\n**4. Business Outcomes:** Reduce missed-update support tickets by 40%; improve 30-day retention by 5%.\n\n**5. Solutions:** Smart digest (daily summary), priority filters, snooze controls.\n**6. Hypotheses:**\n  - We believe that a priority-filtered digest for power users\n    will reduce missed-update tickets as measured by support volume.\n**7. MVP Experiment:** Release digest to 10% of power users for 2 weeks; measure ticket rate vs. control.\n**8. Learning Metric:** Support tickets per active user; target ≥ 30% reduction in cohort.",
    "language": "markdown",
    "description": "eight-field canvas, outcomes drive solutions"
  },
  {
    "ruleId": "",
    "ruleTitle": "Opportunity Solution Tree",
    "type": "bad",
    "code": "Outcome: Increase 30-day retention\n\nSolutions:\n- Build onboarding checklist\n- Add email drip campaign\n- Add progress badges",
    "language": "markdown",
    "description": "outcome jumps directly to solutions, no opportunity mapping"
  },
  {
    "ruleId": "",
    "ruleTitle": "Opportunity Solution Tree",
    "type": "good",
    "code": "Desired Outcome: Increase 30-day retention by 8 points\n\nOpportunities (from 12 customer interviews):\n├── O1: Users don't understand what \"done\" looks like in week 1\n│   ├── Solution A: Onboarding checklist with explicit success milestones\n│   │   └── Experiment: Fake-door test — show checklist to 50% of new signups; measure completion rate\n│   └── Solution B: Welcome call for high-intent signups\n│       └── Experiment: Concierge — manually call 10 users; track 7-day activation\n├── O2: Users lose context after returning from multi-day absence\n│   └── Solution A: \"Pick up where you left off\" re-engagement screen\n│       └── Experiment: Wizard of Oz — manually send personalized re-entry emails; measure return rate\n└── O3: Power features discovered too late to influence habit formation\n    └── Solution A: Contextual feature tips triggered at day 3 and day 7\n        └── Experiment: A/B test tip timing; measure feature adoption rate\n\nNext: Run experiments for O1 in parallel — highest churn signal.",
    "language": "markdown",
    "description": "four-level tree, opportunities surfaced before solutions"
  },
  {
    "ruleId": "",
    "ruleTitle": "PoL Probe Type Advisor",
    "type": "bad",
    "code": "Hypothesis: Enterprise users want an audit log feature.\nProbe: We'll build a landing page and run Google Ads.\n\n[Problem: existing users are already in-product;\na landing page tests acquisition intent, not feature demand from current users.]",
    "language": "markdown",
    "description": "probe type chosen by familiarity, not fit"
  },
  {
    "ruleId": "",
    "ruleTitle": "PoL Probe Type Advisor",
    "type": "good",
    "code": "## Probe Type Advisor: Audit Log Feature\n\n**Step 1 — Hypothesis type:** Solution hypothesis (we know the problem; testing whether\nthis specific solution is wanted).\n\n**Step 2 — Risk level:** High. Engineering cost is 3 sprints; no comparable data exists.\n\n**Step 3 — Available resources:** Existing product surface with enterprise users; no\nengineering capacity for a prototype; PM can send targeted in-app messages.\n\n**Advisor output: Fake Door**\nRationale: Solution hypothesis + high risk + existing product surface = fake door.\nPlace an \"Audit Log\" menu item in the admin panel. Clicking shows: \"Audit log is\nin development — notify me when it's ready.\" Measure click rate among admin users\nover 2 weeks. Success bar: ≥ 15% of admin users engage.\n\n**Ruled out:**\n- Landing page: audience is existing users, not acquisition prospects.\n- Concierge: PM can't manually produce audit logs at enterprise data volumes.\n- Wizard of Oz: requires engineering to fake the log output — not available.",
    "language": "markdown",
    "description": "advisor applied — hypothesis type → risk → resources → probe type"
  },
  {
    "ruleId": "",
    "ruleTitle": "Proof of Learning (PoL) Probe",
    "type": "bad",
    "code": "Plan: Build the team collaboration feature.\nTimeline: 6-week sprint.\nGoal: See if users like it.",
    "language": "markdown",
    "description": "full build to learn, no hypothesis, no criteria"
  },
  {
    "ruleId": "",
    "ruleTitle": "Proof of Learning (PoL) Probe",
    "type": "good",
    "code": "## PoL Probe: Team Collaboration Demand\n\n**Hypothesis:** Mid-market users (5–50 seats) will invite at least one teammate\nwithin 7 days of activation if a prominent \"Invite your team\" CTA exists.\n\n**Probe Type:** Fake door\n- Add \"Invite your team\" button to dashboard (no backend built).\n- Clicking → modal: \"Team features coming soon. Want early access?\"\n- Record click-through rate and early-access sign-ups.\n\n**Success Criteria:** ≥ 20% of eligible users click within 7 days.\n**Timeline:** 5 days to instrument, 2-week measurement window.\n\n**If confirmed (≥ 20%):** Prioritize collaboration feature in next planning cycle.\n**If invalidated (< 20%):** Defer feature; explore whether the barrier is awareness\nor actual need via 5 follow-up interviews.\n\n**Learning owner:** [PM name] presents findings at next team retrospective.",
    "language": "markdown",
    "description": "probe-first, falsifiable hypothesis, explicit success bar"
  },
  {
    "ruleId": "",
    "ruleTitle": "AI Recommendation Canvas",
    "type": "bad",
    "code": "Feature: \"Recommended for you\" section in dashboard.\nEngineering: ML model trained on clickstream data.\nShip date: Next sprint.",
    "language": "markdown",
    "description": "AI feature shipped with no documentation of logic or risk"
  },
  {
    "ruleId": "",
    "ruleTitle": "AI Recommendation Canvas",
    "type": "good",
    "code": "## AI Recommendation Canvas: \"Next Action\" Widget\n\n**Context:**\nUser is a mid-market PM, 14 days post-activation, has created 2 projects but\nhas not invited teammates. Session frequency: 3x/week. Last action: viewed pricing page.\n\n**Approach:**\nCollaborative filtering model trained on behavioral sequences of users who\nreached \"activated\" state (defined as 5+ team members, 3+ projects in 30 days).\nModel outputs top-3 recommended next actions ranked by predicted activation probability.\n\n**Evidence:**\n- Trained on 90-day cohort of 4,200 users; held-out AUC: 0.74.\n- Offline simulation: top-1 recommendation matched actual next action in 41% of cases.\n- Qualitative: 8/10 users in usability test found recommendations \"relevant or very relevant.\"\n\n**Risks:**\n- Cold start: model underperforms for users with < 5 sessions (affects ~30% of new signups).\n  Mitigation: fall back to rule-based recommendations for cold-start users.\n- Feedback loop: if users only click recommended actions, model may narrow over time.\n  Mitigation: inject 10% diversity actions; monitor recommendation entropy monthly.\n- Bias: free-tier users are underrepresented in training data (8% of cohort).\n  Mitigation: stratified sampling in next training run; flag free-tier outputs for review.\n\n**Next Steps:**\n- Week 1–2: A/B test (50/50 split); primary metric = 30-day activation rate.\n- Week 3: Review recommendation distribution for cold-start cohort.\n- Month 2: Retrain model with expanded free-tier sample.\n- Owner: [PM name] reviews recommendation quality dashboard weekly.",
    "language": "markdown",
    "description": "five-field canvas, risks and monitoring owned"
  },
  {
    "ruleId": "",
    "ruleTitle": "6-Frame User Journey Storyboard",
    "type": "bad",
    "code": "Story: User opens app → sees dashboard → clicks \"Create project\" → project is created.",
    "language": "markdown",
    "description": "product-centric, no emotional arc, no trigger moment"
  },
  {
    "ruleId": "",
    "ruleTitle": "6-Frame User Journey Storyboard",
    "type": "good",
    "code": "## Storyboard: \"The Monday Morning Panic\" — Project Onboarding\n\n**Frame 1 — Setup:**\nPriya is a team lead at a 40-person SaaS company. She manages 3 active projects\nacross 6 people. She uses spreadsheets, Slack, and two other tools. Her team\nmisses context constantly.\n\n**Frame 2 — Trigger:**\nMonday 9:05am. A stakeholder pings: \"What's the status on the migration?\"\nPriya has no single source of truth. She spends 20 minutes reconstructing the\nanswer from 4 different threads. She is embarrassed and frustrated.\n\n**Frame 3 — Action:**\nA colleague shares a link: \"We moved everything into [Product]. Here's the\nworkspace.\" Priya clicks, sees her projects already partially populated\n(the PM had imported them from Jira). She spends 8 minutes reviewing.\n\n**Frame 4 — Resolution:**\nAt 9:35am, the stakeholder pings again. Priya pastes a direct link to the\nlive status board. No reconstruction needed. The stakeholder says \"perfect.\"\n\n**Frame 5 — Outcome:**\nPriya blocks 30 minutes Friday afternoon to keep the board updated.\nShe tells two other team leads to try it. Monday meetings are now 15 minutes shorter.\n\n**Frame 6 — Insight:**\nThe trigger is not \"lack of a project tool\" — it is \"stakeholder pressure without\na defensible answer.\" The product must make Priya look competent to her stakeholders,\nnot just organized to herself. This reframes the onboarding copy and the share flow.\n\nWhat must be true: Import from existing tools (Jira, spreadsheets) must work in\n< 10 minutes or Priya will not reach Frame 3.",
    "language": "markdown",
    "description": "6-frame narrative, grounded in user context and emotion"
  }
]